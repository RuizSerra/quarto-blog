{"title":"Review article: Inverse Reinforcement Learning as the Algorithmic Basis for Theory of Mind","markdown":{"yaml":{"title":"Review article: Inverse Reinforcement Learning as the Algorithmic Basis for Theory of Mind","author":"Jaime Ruiz Serra","date":"2023-01-19","categories":["publications"],"image":"paper-thumbnail.jpeg"},"containsRefs":false,"markdown":"\n\n> Theory of mind (ToM) is the psychological construct by which we model anotherâ€™s internal mental states. Through ToM, we adjust our own behaviour to best suit a social context, and therefore it is essential to our everyday interactions with others. In adopting an algorithmic (rather than a psychological or neurological) approach to ToM, we gain insights into cognition that will aid us in building more accurate models for the cognitive and behavioural sciences, as well as enable artificial agents to be more proficient in social interactions as they become more embedded in our everyday lives. Inverse reinforcement learning (IRL) is a class of machine learning methods by which to infer the preferences (rewards as a function of state) of a decision maker from its behaviour (trajectories in a Markov decision process). IRL can provide a computational approach for ToM, as recently outlined by Jara-Ettinger, but this will require a better understanding of the relationship between ToM concepts and existing IRL methods at the algorthmic level. Here, we provide a review of prominent IRL algorithms and their formal descriptions, and discuss the applicability of IRL concepts as the algorithmic basis of a ToM in AI.\n\nPublished in MDPI Algorithms, Open Access, [DOI:10.3390/a16020068](https://doi.org/10.3390/a16020068).\n\nCitation:\n```\n@article{Ruiz-Serra2023InverseReinforcement,\n  title = {Inverse {{Reinforcement Learning}} as the {{Algorithmic Basis}} for {{Theory}} of {{Mind}}: {{Current Methods}} and {{Open Problems}}},\n  shorttitle = {Inverse {{Reinforcement Learning}} as the {{Algorithmic Basis}} for {{Theory}} of {{Mind}}},\n  author = {{Ruiz-Serra}, Jaime and Harr{\\'e}, Michael S.},\n  year = {2023},\n  month = feb,\n  journal = {Algorithms},\n  volume = {16},\n  number = {2},\n  pages = {68},\n  publisher = {{Multidisciplinary Digital Publishing Institute}},\n  issn = {1999-4893},\n  doi = {10.3390/a16020068}\n}\n```"},"formats":{"html":{"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"default","error":false,"eval":true,"cache":null,"freeze":true,"echo":true,"output":true,"warning":true,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"engine":"markdown"},"render":{"keep-tex":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[]},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","css":["../../styles.css"],"output-file":"index.html"},"language":{},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.2.280","theme":{"light":["sandstone","../../styles.scss"],"dark":"darkly"},"title-block-banner":true,"title":"Review article: Inverse Reinforcement Learning as the Algorithmic Basis for Theory of Mind","author":"Jaime Ruiz Serra","date":"2023-01-19","categories":["publications"],"image":"paper-thumbnail.jpeg"},"extensions":{"book":{"multiFile":true}}}}}